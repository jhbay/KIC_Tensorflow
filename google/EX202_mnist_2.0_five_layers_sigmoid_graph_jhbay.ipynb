{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.8.0\n",
      "training :  0  accuracy =   0.0600  loss =  231.403\n",
      "testing  :  0  accuracy =   0.0980  loss =  230.531\n",
      "training :  1  accuracy =   0.0700  loss =  229.642\n",
      "testing  :  1  accuracy =   0.0958  loss =  230.449\n",
      "training :  2  accuracy =   0.1200  loss =  229.27\n",
      "testing  :  2  accuracy =   0.0980  loss =  230.407\n",
      "training :  3  accuracy =   0.1400  loss =  230.795\n",
      "testing  :  3  accuracy =   0.1028  loss =  230.408\n",
      "training :  4  accuracy =   0.0500  loss =  231.64\n",
      "testing  :  4  accuracy =   0.1028  loss =  230.405\n",
      "training :  5  accuracy =   0.0400  loss =  232.221\n",
      "testing  :  5  accuracy =   0.1028  loss =  230.389\n",
      "training :  6  accuracy =   0.0700  loss =  231.317\n",
      "testing  :  6  accuracy =   0.1028  loss =  230.348\n",
      "training :  7  accuracy =   0.1600  loss =  229.333\n",
      "testing  :  7  accuracy =   0.1028  loss =  230.295\n",
      "training :  8  accuracy =   0.0900  loss =  230.846\n",
      "testing  :  8  accuracy =   0.1028  loss =  230.252\n",
      "training :  9  accuracy =   0.0800  loss =  229.981\n",
      "testing  :  9  accuracy =   0.1028  loss =  230.202\n",
      "training :  10  accuracy =   0.1300  loss =  229.776\n",
      "testing  :  10  accuracy =   0.1028  loss =  230.153\n",
      "training :  11  accuracy =   0.0900  loss =  230.821\n",
      "testing  :  11  accuracy =   0.1028  loss =  230.107\n",
      "training :  12  accuracy =   0.1400  loss =  229.701\n",
      "testing  :  12  accuracy =   0.1028  loss =  230.063\n",
      "training :  13  accuracy =   0.0300  loss =  230.49\n",
      "testing  :  13  accuracy =   0.1028  loss =  230.018\n",
      "training :  14  accuracy =   0.2400  loss =  229.435\n",
      "testing  :  14  accuracy =   0.2104  loss =  229.964\n",
      "training :  15  accuracy =   0.2600  loss =  229.666\n",
      "testing  :  15  accuracy =   0.1672  loss =  229.914\n",
      "training :  16  accuracy =   0.0900  loss =  230.553\n",
      "testing  :  16  accuracy =   0.1135  loss =  229.859\n",
      "training :  17  accuracy =   0.1200  loss =  229.565\n",
      "testing  :  17  accuracy =   0.1135  loss =  229.803\n",
      "training :  18  accuracy =   0.1400  loss =  228.86\n",
      "testing  :  18  accuracy =   0.1135  loss =  229.745\n",
      "training :  19  accuracy =   0.1400  loss =  229.413\n",
      "testing  :  19  accuracy =   0.1135  loss =  229.68\n",
      "training :  20  accuracy =   0.1000  loss =  229.639\n",
      "testing  :  20  accuracy =   0.1135  loss =  229.609\n",
      "training :  21  accuracy =   0.1100  loss =  228.739\n",
      "testing  :  21  accuracy =   0.1135  loss =  229.527\n",
      "training :  22  accuracy =   0.1400  loss =  229.417\n",
      "testing  :  22  accuracy =   0.1135  loss =  229.438\n",
      "training :  23  accuracy =   0.1300  loss =  229.516\n",
      "testing  :  23  accuracy =   0.1135  loss =  229.345\n",
      "training :  24  accuracy =   0.1200  loss =  229.157\n",
      "testing  :  24  accuracy =   0.1135  loss =  229.244\n",
      "training :  25  accuracy =   0.1300  loss =  229.016\n",
      "testing  :  25  accuracy =   0.1135  loss =  229.132\n",
      "training :  26  accuracy =   0.1000  loss =  228.837\n",
      "testing  :  26  accuracy =   0.1135  loss =  229.005\n",
      "training :  27  accuracy =   0.0700  loss =  229.176\n",
      "testing  :  27  accuracy =   0.1135  loss =  228.865\n",
      "training :  28  accuracy =   0.1400  loss =  228.171\n",
      "testing  :  28  accuracy =   0.1135  loss =  228.71\n",
      "training :  29  accuracy =   0.1400  loss =  229.197\n",
      "testing  :  29  accuracy =   0.1135  loss =  228.542\n",
      "training :  30  accuracy =   0.0700  loss =  229.083\n",
      "testing  :  30  accuracy =   0.1135  loss =  228.357\n",
      "training :  31  accuracy =   0.1500  loss =  227.459\n",
      "testing  :  31  accuracy =   0.1135  loss =  228.157\n",
      "training :  32  accuracy =   0.2000  loss =  227.252\n",
      "testing  :  32  accuracy =   0.1506  loss =  227.942\n",
      "training :  33  accuracy =   0.1400  loss =  228.58\n",
      "testing  :  33  accuracy =   0.1757  loss =  227.709\n",
      "training :  34  accuracy =   0.1800  loss =  227.39\n",
      "testing  :  34  accuracy =   0.1879  loss =  227.454\n",
      "training :  35  accuracy =   0.2100  loss =  226.953\n",
      "testing  :  35  accuracy =   0.1912  loss =  227.174\n",
      "training :  36  accuracy =   0.1800  loss =  227.529\n",
      "testing  :  36  accuracy =   0.1947  loss =  226.877\n",
      "training :  37  accuracy =   0.2100  loss =  225.925\n",
      "testing  :  37  accuracy =   0.1968  loss =  226.569\n",
      "training :  38  accuracy =   0.1300  loss =  226.993\n",
      "testing  :  38  accuracy =   0.1988  loss =  226.238\n",
      "training :  39  accuracy =   0.2000  loss =  226.524\n",
      "testing  :  39  accuracy =   0.2008  loss =  225.901\n",
      "training :  40  accuracy =   0.1600  loss =  225.612\n",
      "testing  :  40  accuracy =   0.2030  loss =  225.533\n",
      "training :  41  accuracy =   0.2400  loss =  224.867\n",
      "testing  :  41  accuracy =   0.2047  loss =  225.144\n",
      "training :  42  accuracy =   0.2800  loss =  223.778\n",
      "testing  :  42  accuracy =   0.2152  loss =  224.762\n",
      "training :  43  accuracy =   0.2500  loss =  224.45\n",
      "testing  :  43  accuracy =   0.2433  loss =  224.363\n",
      "training :  44  accuracy =   0.3000  loss =  222.951\n",
      "testing  :  44  accuracy =   0.2571  loss =  223.895\n",
      "training :  45  accuracy =   0.3900  loss =  221.657\n",
      "testing  :  45  accuracy =   0.2681  loss =  223.379\n",
      "training :  46  accuracy =   0.2700  loss =  221.777\n",
      "testing  :  46  accuracy =   0.2680  loss =  222.888\n",
      "training :  47  accuracy =   0.2400  loss =  222.567\n",
      "testing  :  47  accuracy =   0.2690  loss =  222.394\n",
      "training :  48  accuracy =   0.2600  loss =  221.366\n",
      "testing  :  48  accuracy =   0.2778  loss =  221.874\n",
      "training :  49  accuracy =   0.3400  loss =  221.347\n",
      "testing  :  49  accuracy =   0.2906  loss =  221.326\n",
      "training :  50  accuracy =   0.2700  loss =  221.205\n",
      "testing  :  50  accuracy =   0.3080  loss =  220.753\n",
      "training :  51  accuracy =   0.3700  loss =  219.086\n",
      "testing  :  51  accuracy =   0.3415  loss =  220.128\n",
      "training :  52  accuracy =   0.4200  loss =  219.313\n",
      "testing  :  52  accuracy =   0.3681  loss =  219.475\n",
      "training :  53  accuracy =   0.3900  loss =  219.232\n",
      "testing  :  53  accuracy =   0.3805  loss =  218.841\n",
      "training :  54  accuracy =   0.2900  loss =  219.338\n",
      "testing  :  54  accuracy =   0.3862  loss =  218.232\n",
      "training :  55  accuracy =   0.3300  loss =  218.964\n",
      "testing  :  55  accuracy =   0.3903  loss =  217.64\n",
      "training :  56  accuracy =   0.4700  loss =  216.015\n",
      "testing  :  56  accuracy =   0.4046  loss =  217.021\n",
      "training :  57  accuracy =   0.3700  loss =  216.973\n",
      "testing  :  57  accuracy =   0.4269  loss =  216.328\n",
      "training :  58  accuracy =   0.4500  loss =  215.707\n",
      "testing  :  58  accuracy =   0.4398  loss =  215.667\n",
      "training :  59  accuracy =   0.3800  loss =  216.172\n",
      "testing  :  59  accuracy =   0.4443  loss =  215.022\n",
      "training :  60  accuracy =   0.4500  loss =  214.939\n",
      "testing  :  60  accuracy =   0.4532  loss =  214.39\n",
      "training :  61  accuracy =   0.4500  loss =  213.96\n",
      "testing  :  61  accuracy =   0.4572  loss =  213.727\n",
      "training :  62  accuracy =   0.5000  loss =  211.647\n",
      "testing  :  62  accuracy =   0.4560  loss =  213.075\n",
      "training :  63  accuracy =   0.5200  loss =  211.528\n",
      "testing  :  63  accuracy =   0.4541  loss =  212.458\n",
      "training :  64  accuracy =   0.4700  loss =  210.753\n",
      "testing  :  64  accuracy =   0.4522  loss =  211.853\n",
      "training :  65  accuracy =   0.4100  loss =  211.13\n",
      "testing  :  65  accuracy =   0.4542  loss =  211.167\n",
      "training :  66  accuracy =   0.4800  loss =  210.22\n",
      "testing  :  66  accuracy =   0.4555  loss =  210.531\n",
      "training :  67  accuracy =   0.4800  loss =  209.184\n",
      "testing  :  67  accuracy =   0.4568  loss =  209.962\n",
      "training :  68  accuracy =   0.4700  loss =  208.012\n",
      "testing  :  68  accuracy =   0.4604  loss =  209.375\n",
      "training :  69  accuracy =   0.5400  loss =  206.155\n",
      "testing  :  69  accuracy =   0.4630  loss =  208.776\n",
      "training :  70  accuracy =   0.4400  loss =  209.905\n",
      "testing  :  70  accuracy =   0.4661  loss =  208.146\n",
      "training :  71  accuracy =   0.4400  loss =  208.975\n",
      "testing  :  71  accuracy =   0.4669  loss =  207.47\n",
      "training :  72  accuracy =   0.4100  loss =  206.768\n",
      "testing  :  72  accuracy =   0.4654  loss =  206.897\n",
      "training :  73  accuracy =   0.5600  loss =  204.807\n",
      "testing  :  73  accuracy =   0.4652  loss =  206.323\n",
      "training :  74  accuracy =   0.4100  loss =  206.942\n",
      "testing  :  74  accuracy =   0.4671  loss =  205.738\n",
      "training :  75  accuracy =   0.5100  loss =  202.783\n",
      "testing  :  75  accuracy =   0.4670  loss =  205.188\n",
      "training :  76  accuracy =   0.5200  loss =  204.187\n",
      "testing  :  76  accuracy =   0.4674  loss =  204.686\n",
      "training :  77  accuracy =   0.3900  loss =  204.278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing  :  77  accuracy =   0.4690  loss =  204.191\n",
      "training :  78  accuracy =   0.4200  loss =  203.605\n",
      "testing  :  78  accuracy =   0.4693  loss =  203.667\n",
      "training :  79  accuracy =   0.4700  loss =  201.21\n",
      "testing  :  79  accuracy =   0.4708  loss =  203.116\n",
      "training :  80  accuracy =   0.4000  loss =  203.58\n",
      "testing  :  80  accuracy =   0.4712  loss =  202.622\n",
      "training :  81  accuracy =   0.4400  loss =  199.897\n",
      "testing  :  81  accuracy =   0.4695  loss =  202.22\n",
      "training :  82  accuracy =   0.4400  loss =  202.03\n",
      "testing  :  82  accuracy =   0.4703  loss =  201.85\n",
      "training :  83  accuracy =   0.4500  loss =  200.132\n",
      "testing  :  83  accuracy =   0.4729  loss =  201.343\n",
      "training :  84  accuracy =   0.5400  loss =  197.929\n",
      "testing  :  84  accuracy =   0.4781  loss =  200.838\n",
      "training :  85  accuracy =   0.4800  loss =  201.505\n",
      "testing  :  85  accuracy =   0.4780  loss =  200.376\n",
      "training :  86  accuracy =   0.5200  loss =  199.991\n",
      "testing  :  86  accuracy =   0.4766  loss =  200.003\n",
      "training :  87  accuracy =   0.5300  loss =  198.967\n",
      "testing  :  87  accuracy =   0.4763  loss =  199.735\n",
      "training :  88  accuracy =   0.5400  loss =  198.06\n",
      "testing  :  88  accuracy =   0.4734  loss =  199.513\n",
      "training :  89  accuracy =   0.4600  loss =  200.331\n",
      "testing  :  89  accuracy =   0.4737  loss =  199.152\n",
      "training :  90  accuracy =   0.4600  loss =  199.582\n",
      "testing  :  90  accuracy =   0.4750  loss =  198.662\n",
      "training :  91  accuracy =   0.4200  loss =  199.457\n",
      "testing  :  91  accuracy =   0.4754  loss =  198.122\n",
      "training :  92  accuracy =   0.5400  loss =  196.045\n",
      "testing  :  92  accuracy =   0.4782  loss =  197.678\n",
      "training :  93  accuracy =   0.5300  loss =  197.766\n",
      "testing  :  93  accuracy =   0.4800  loss =  197.354\n",
      "training :  94  accuracy =   0.4800  loss =  195.905\n",
      "testing  :  94  accuracy =   0.4838  loss =  197.081\n",
      "training :  95  accuracy =   0.4400  loss =  196.284\n",
      "testing  :  95  accuracy =   0.4885  loss =  196.772\n",
      "training :  96  accuracy =   0.5600  loss =  196.714\n",
      "testing  :  96  accuracy =   0.4956  loss =  196.487\n",
      "training :  97  accuracy =   0.5800  loss =  194.439\n",
      "testing  :  97  accuracy =   0.5013  loss =  196.124\n",
      "training :  98  accuracy =   0.4000  loss =  196.82\n",
      "testing  :  98  accuracy =   0.5094  loss =  195.789\n",
      "training :  99  accuracy =   0.6200  loss =  193.36\n",
      "testing  :  99  accuracy =   0.5071  loss =  195.413\n",
      "training :  100  accuracy =   0.4200  loss =  194.351\n",
      "testing  :  100  accuracy =   0.5010  loss =  195.117\n",
      "training :  101  accuracy =   0.4800  loss =  194.443\n",
      "testing  :  101  accuracy =   0.4952  loss =  194.919\n",
      "training :  102  accuracy =   0.5200  loss =  194.249\n",
      "testing  :  102  accuracy =   0.5007  loss =  194.671\n",
      "training :  103  accuracy =   0.5700  loss =  193.731\n",
      "testing  :  103  accuracy =   0.5117  loss =  194.392\n",
      "training :  104  accuracy =   0.4700  loss =  196.17\n",
      "testing  :  104  accuracy =   0.5275  loss =  194.097\n",
      "training :  105  accuracy =   0.6100  loss =  193.532\n",
      "testing  :  105  accuracy =   0.5398  loss =  193.708\n",
      "training :  106  accuracy =   0.5500  loss =  191.935\n",
      "testing  :  106  accuracy =   0.5428  loss =  193.409\n",
      "training :  107  accuracy =   0.4700  loss =  192.85\n",
      "testing  :  107  accuracy =   0.5348  loss =  193.246\n",
      "training :  108  accuracy =   0.5200  loss =  192.383\n",
      "testing  :  108  accuracy =   0.5299  loss =  193.077\n",
      "training :  109  accuracy =   0.5700  loss =  192.169\n",
      "testing  :  109  accuracy =   0.5242  loss =  192.93\n",
      "training :  110  accuracy =   0.6000  loss =  192.116\n",
      "testing  :  110  accuracy =   0.5259  loss =  192.636\n",
      "training :  111  accuracy =   0.5400  loss =  192.095\n",
      "testing  :  111  accuracy =   0.5240  loss =  192.235\n",
      "training :  112  accuracy =   0.5000  loss =  192.784\n",
      "testing  :  112  accuracy =   0.5055  loss =  191.889\n",
      "training :  113  accuracy =   0.4000  loss =  190.825\n",
      "testing  :  113  accuracy =   0.4947  loss =  191.571\n",
      "training :  114  accuracy =   0.5600  loss =  191.989\n",
      "testing  :  114  accuracy =   0.4890  loss =  191.29\n",
      "training :  115  accuracy =   0.5300  loss =  187.398\n",
      "testing  :  115  accuracy =   0.4840  loss =  191.058\n",
      "training :  116  accuracy =   0.4700  loss =  189.132\n",
      "testing  :  116  accuracy =   0.4819  loss =  190.886\n",
      "training :  117  accuracy =   0.4700  loss =  190.698\n",
      "testing  :  117  accuracy =   0.4795  loss =  190.733\n",
      "training :  118  accuracy =   0.5200  loss =  189.127\n",
      "testing  :  118  accuracy =   0.4784  loss =  190.573\n",
      "training :  119  accuracy =   0.5400  loss =  187.455\n",
      "testing  :  119  accuracy =   0.4782  loss =  190.376\n",
      "training :  120  accuracy =   0.5100  loss =  189.443\n",
      "testing  :  120  accuracy =   0.4773  loss =  190.226\n",
      "training :  121  accuracy =   0.4600  loss =  192.202\n",
      "testing  :  121  accuracy =   0.4785  loss =  189.995\n",
      "training :  122  accuracy =   0.5400  loss =  186.312\n",
      "testing  :  122  accuracy =   0.4817  loss =  189.679\n",
      "training :  123  accuracy =   0.4000  loss =  190.822\n",
      "testing  :  123  accuracy =   0.4842  loss =  189.424\n",
      "training :  124  accuracy =   0.4600  loss =  187.436\n",
      "testing  :  124  accuracy =   0.4862  loss =  189.192\n",
      "training :  125  accuracy =   0.4600  loss =  192.842\n",
      "testing  :  125  accuracy =   0.4855  loss =  189.062\n",
      "training :  126  accuracy =   0.4600  loss =  189.143\n",
      "testing  :  126  accuracy =   0.4888  loss =  188.856\n",
      "training :  127  accuracy =   0.5600  loss =  186.85\n",
      "testing  :  127  accuracy =   0.5054  loss =  188.62\n",
      "training :  128  accuracy =   0.4800  loss =  189.178\n",
      "testing  :  128  accuracy =   0.4723  loss =  188.427\n",
      "training :  129  accuracy =   0.5000  loss =  186.373\n",
      "testing  :  129  accuracy =   0.4736  loss =  188.292\n",
      "training :  130  accuracy =   0.5200  loss =  191.372\n",
      "testing  :  130  accuracy =   0.4732  loss =  188.183\n",
      "training :  131  accuracy =   0.4300  loss =  188.503\n",
      "testing  :  131  accuracy =   0.4733  loss =  188.087\n",
      "training :  132  accuracy =   0.4800  loss =  185.463\n",
      "testing  :  132  accuracy =   0.4704  loss =  187.882\n",
      "training :  133  accuracy =   0.4800  loss =  187.599\n",
      "testing  :  133  accuracy =   0.4674  loss =  187.697\n",
      "training :  134  accuracy =   0.4500  loss =  186.767\n",
      "testing  :  134  accuracy =   0.4667  loss =  187.481\n",
      "training :  135  accuracy =   0.4500  loss =  187.173\n",
      "testing  :  135  accuracy =   0.4645  loss =  187.355\n",
      "training :  136  accuracy =   0.5100  loss =  191.233\n",
      "testing  :  136  accuracy =   0.4635  loss =  187.223\n",
      "training :  137  accuracy =   0.4500  loss =  187.121\n",
      "testing  :  137  accuracy =   0.4676  loss =  186.975\n",
      "training :  138  accuracy =   0.3900  loss =  186.197\n",
      "testing  :  138  accuracy =   0.4701  loss =  186.774\n",
      "training :  139  accuracy =   0.5100  loss =  186.056\n",
      "testing  :  139  accuracy =   0.4733  loss =  186.631\n",
      "training :  140  accuracy =   0.4100  loss =  185.908\n",
      "testing  :  140  accuracy =   0.4755  loss =  186.485\n",
      "training :  141  accuracy =   0.4700  loss =  186.795\n",
      "testing  :  141  accuracy =   0.4787  loss =  186.336\n",
      "training :  142  accuracy =   0.4500  loss =  185.285\n",
      "testing  :  142  accuracy =   0.4799  loss =  186.161\n",
      "training :  143  accuracy =   0.4600  loss =  184.232\n",
      "testing  :  143  accuracy =   0.4804  loss =  185.979\n",
      "training :  144  accuracy =   0.4000  loss =  189.03\n",
      "testing  :  144  accuracy =   0.4834  loss =  185.823\n",
      "training :  145  accuracy =   0.4100  loss =  186.294\n",
      "testing  :  145  accuracy =   0.4850  loss =  185.702\n",
      "training :  146  accuracy =   0.4500  loss =  184.51\n",
      "testing  :  146  accuracy =   0.4891  loss =  185.561\n",
      "training :  147  accuracy =   0.4900  loss =  186.068\n",
      "testing  :  147  accuracy =   0.4952  loss =  185.448\n",
      "training :  148  accuracy =   0.4500  loss =  189.403\n",
      "testing  :  148  accuracy =   0.5031  loss =  185.366\n",
      "training :  149  accuracy =   0.4900  loss =  183.34\n",
      "testing  :  149  accuracy =   0.5032  loss =  185.22\n",
      "training :  150  accuracy =   0.4800  loss =  188.215\n",
      "testing  :  150  accuracy =   0.5037  loss =  185.104\n",
      "training :  151  accuracy =   0.5600  loss =  185.641\n",
      "testing  :  151  accuracy =   0.4936  loss =  184.978\n",
      "training :  152  accuracy =   0.5200  loss =  184.438\n",
      "testing  :  152  accuracy =   0.4864  loss =  184.853\n",
      "training :  153  accuracy =   0.5300  loss =  182.664\n",
      "testing  :  153  accuracy =   0.4825  loss =  184.71\n",
      "training :  154  accuracy =   0.5000  loss =  185.204\n",
      "testing  :  154  accuracy =   0.4750  loss =  184.555\n",
      "training :  155  accuracy =   0.4500  loss =  185.856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing  :  155  accuracy =   0.4672  loss =  184.409\n",
      "training :  156  accuracy =   0.4700  loss =  182.098\n",
      "testing  :  156  accuracy =   0.4654  loss =  184.273\n",
      "training :  157  accuracy =   0.5100  loss =  182.857\n",
      "testing  :  157  accuracy =   0.4607  loss =  184.2\n",
      "training :  158  accuracy =   0.4900  loss =  180.812\n",
      "testing  :  158  accuracy =   0.4536  loss =  184.222\n",
      "training :  159  accuracy =   0.5000  loss =  185.956\n",
      "testing  :  159  accuracy =   0.4482  loss =  184.303\n",
      "training :  160  accuracy =   0.5000  loss =  183.368\n",
      "testing  :  160  accuracy =   0.4394  loss =  184.601\n",
      "training :  161  accuracy =   0.4800  loss =  180.701\n",
      "testing  :  161  accuracy =   0.4346  loss =  184.696\n",
      "training :  162  accuracy =   0.4600  loss =  182.496\n",
      "testing  :  162  accuracy =   0.4319  loss =  184.573\n",
      "training :  163  accuracy =   0.3700  loss =  187.634\n",
      "testing  :  163  accuracy =   0.4329  loss =  184.155\n",
      "training :  164  accuracy =   0.4300  loss =  184.997\n",
      "testing  :  164  accuracy =   0.4376  loss =  183.626\n",
      "training :  165  accuracy =   0.4100  loss =  185.5\n",
      "testing  :  165  accuracy =   0.4397  loss =  183.296\n",
      "training :  166  accuracy =   0.4700  loss =  183.341\n",
      "testing  :  166  accuracy =   0.4403  loss =  183.11\n",
      "training :  167  accuracy =   0.4800  loss =  182.687\n",
      "testing  :  167  accuracy =   0.4435  loss =  183.096\n",
      "training :  168  accuracy =   0.4700  loss =  185.042\n",
      "testing  :  168  accuracy =   0.4543  loss =  183.161\n",
      "training :  169  accuracy =   0.4100  loss =  183.338\n",
      "testing  :  169  accuracy =   0.4644  loss =  183.181\n",
      "training :  170  accuracy =   0.4100  loss =  182.989\n",
      "testing  :  170  accuracy =   0.4877  loss =  183.251\n",
      "training :  171  accuracy =   0.4900  loss =  181.618\n",
      "testing  :  171  accuracy =   0.4874  loss =  183.063\n",
      "training :  172  accuracy =   0.5500  loss =  182.877\n",
      "testing  :  172  accuracy =   0.4875  loss =  182.836\n",
      "training :  173  accuracy =   0.4500  loss =  183.007\n",
      "testing  :  173  accuracy =   0.4846  loss =  182.559\n",
      "training :  174  accuracy =   0.5100  loss =  183.766\n",
      "testing  :  174  accuracy =   0.4900  loss =  182.464\n",
      "training :  175  accuracy =   0.5100  loss =  180.716\n",
      "testing  :  175  accuracy =   0.4855  loss =  182.64\n",
      "training :  176  accuracy =   0.5200  loss =  187.076\n",
      "testing  :  176  accuracy =   0.4825  loss =  183.003\n",
      "training :  177  accuracy =   0.4600  loss =  187.159\n",
      "testing  :  177  accuracy =   0.4782  loss =  183.17\n",
      "training :  178  accuracy =   0.4200  loss =  183.199\n",
      "testing  :  178  accuracy =   0.4788  loss =  183.203\n",
      "training :  179  accuracy =   0.4600  loss =  185.413\n",
      "testing  :  179  accuracy =   0.4836  loss =  183.072\n",
      "training :  180  accuracy =   0.5500  loss =  182.251\n",
      "testing  :  180  accuracy =   0.4976  loss =  182.58\n",
      "training :  181  accuracy =   0.5100  loss =  182.46\n",
      "testing  :  181  accuracy =   0.5097  loss =  181.893\n",
      "training :  182  accuracy =   0.5800  loss =  180.403\n",
      "testing  :  182  accuracy =   0.5218  loss =  181.472\n",
      "training :  183  accuracy =   0.5400  loss =  181.227\n",
      "testing  :  183  accuracy =   0.5272  loss =  181.399\n",
      "training :  184  accuracy =   0.5200  loss =  182.365\n",
      "testing  :  184  accuracy =   0.5243  loss =  181.462\n",
      "training :  185  accuracy =   0.4100  loss =  184.013\n",
      "testing  :  185  accuracy =   0.5161  loss =  181.598\n",
      "training :  186  accuracy =   0.4900  loss =  181.47\n",
      "testing  :  186  accuracy =   0.5103  loss =  181.628\n",
      "training :  187  accuracy =   0.5200  loss =  181.005\n",
      "testing  :  187  accuracy =   0.5040  loss =  181.592\n",
      "training :  188  accuracy =   0.4200  loss =  183.25\n",
      "testing  :  188  accuracy =   0.5076  loss =  181.422\n",
      "training :  189  accuracy =   0.4600  loss =  182.732\n",
      "testing  :  189  accuracy =   0.5194  loss =  181.205\n",
      "training :  190  accuracy =   0.5000  loss =  180.996\n",
      "testing  :  190  accuracy =   0.5439  loss =  180.926\n",
      "training :  191  accuracy =   0.5100  loss =  181.946\n",
      "testing  :  191  accuracy =   0.5517  loss =  180.787\n",
      "training :  192  accuracy =   0.5100  loss =  180.469\n",
      "testing  :  192  accuracy =   0.5339  loss =  180.84\n",
      "training :  193  accuracy =   0.4600  loss =  181.72\n",
      "testing  :  193  accuracy =   0.5477  loss =  180.874\n",
      "training :  194  accuracy =   0.6100  loss =  180.127\n",
      "testing  :  194  accuracy =   0.5643  loss =  180.829\n",
      "training :  195  accuracy =   0.5600  loss =  182.648\n",
      "testing  :  195  accuracy =   0.5694  loss =  180.885\n",
      "training :  196  accuracy =   0.4900  loss =  178.884\n",
      "testing  :  196  accuracy =   0.5550  loss =  180.776\n",
      "training :  197  accuracy =   0.5700  loss =  181.19\n",
      "testing  :  197  accuracy =   0.5338  loss =  180.752\n",
      "training :  198  accuracy =   0.5000  loss =  179.163\n",
      "testing  :  198  accuracy =   0.4979  loss =  180.579\n",
      "training :  199  accuracy =   0.5200  loss =  181.017\n",
      "testing  :  199  accuracy =   0.4752  loss =  180.448\n",
      "training :  200  accuracy =   0.5100  loss =  178.177\n",
      "testing  :  200  accuracy =   0.4624  loss =  180.267\n",
      "training :  201  accuracy =   0.4500  loss =  180.189\n",
      "testing  :  201  accuracy =   0.4551  loss =  180.076\n",
      "training :  202  accuracy =   0.4300  loss =  183.684\n",
      "testing  :  202  accuracy =   0.4505  loss =  179.948\n",
      "training :  203  accuracy =   0.3400  loss =  178.652\n",
      "testing  :  203  accuracy =   0.4443  loss =  179.851\n",
      "training :  204  accuracy =   0.4800  loss =  181.739\n",
      "testing  :  204  accuracy =   0.4462  loss =  179.807\n",
      "training :  205  accuracy =   0.4600  loss =  178.924\n",
      "testing  :  205  accuracy =   0.4627  loss =  179.874\n",
      "training :  206  accuracy =   0.4000  loss =  178.778\n",
      "testing  :  206  accuracy =   0.4599  loss =  179.8\n",
      "training :  207  accuracy =   0.4000  loss =  180.635\n",
      "testing  :  207  accuracy =   0.4585  loss =  179.724\n",
      "training :  208  accuracy =   0.5200  loss =  176.089\n",
      "testing  :  208  accuracy =   0.4597  loss =  179.653\n",
      "training :  209  accuracy =   0.3600  loss =  178.361\n",
      "testing  :  209  accuracy =   0.4559  loss =  179.578\n",
      "training :  210  accuracy =   0.3900  loss =  182.098\n",
      "testing  :  210  accuracy =   0.4564  loss =  179.471\n",
      "training :  211  accuracy =   0.4800  loss =  178.811\n",
      "testing  :  211  accuracy =   0.4716  loss =  179.273\n",
      "training :  212  accuracy =   0.4900  loss =  179.392\n",
      "testing  :  212  accuracy =   0.5019  loss =  179.078\n",
      "training :  213  accuracy =   0.4800  loss =  180.315\n",
      "testing  :  213  accuracy =   0.4927  loss =  179.029\n",
      "training :  214  accuracy =   0.4400  loss =  179.051\n",
      "testing  :  214  accuracy =   0.4942  loss =  179.052\n",
      "training :  215  accuracy =   0.5200  loss =  178.769\n",
      "testing  :  215  accuracy =   0.4994  loss =  179.069\n",
      "training :  216  accuracy =   0.5200  loss =  178.989\n",
      "testing  :  216  accuracy =   0.5078  loss =  179.116\n",
      "training :  217  accuracy =   0.4700  loss =  179.381\n",
      "testing  :  217  accuracy =   0.5187  loss =  179.109\n",
      "training :  218  accuracy =   0.5000  loss =  181.164\n",
      "testing  :  218  accuracy =   0.5256  loss =  179.041\n",
      "training :  219  accuracy =   0.4300  loss =  179.666\n",
      "testing  :  219  accuracy =   0.5350  loss =  178.905\n",
      "training :  220  accuracy =   0.5700  loss =  177.567\n",
      "testing  :  220  accuracy =   0.5390  loss =  178.857\n",
      "training :  221  accuracy =   0.5900  loss =  176.817\n",
      "testing  :  221  accuracy =   0.5370  loss =  178.797\n",
      "training :  222  accuracy =   0.5000  loss =  177.88\n",
      "testing  :  222  accuracy =   0.5377  loss =  178.751\n",
      "training :  223  accuracy =   0.4700  loss =  178.26\n",
      "testing  :  223  accuracy =   0.5334  loss =  178.756\n",
      "training :  224  accuracy =   0.5600  loss =  179.604\n",
      "testing  :  224  accuracy =   0.5292  loss =  178.653\n",
      "training :  225  accuracy =   0.4900  loss =  179.558\n",
      "testing  :  225  accuracy =   0.5193  loss =  178.522\n",
      "training :  226  accuracy =   0.5200  loss =  182.066\n",
      "testing  :  226  accuracy =   0.5071  loss =  178.397\n",
      "training :  227  accuracy =   0.4400  loss =  178.651\n",
      "testing  :  227  accuracy =   0.4808  loss =  178.429\n",
      "training :  228  accuracy =   0.5900  loss =  175.839\n",
      "testing  :  228  accuracy =   0.4628  loss =  178.807\n",
      "training :  229  accuracy =   0.4500  loss =  179.12\n",
      "testing  :  229  accuracy =   0.4507  loss =  179.295\n",
      "training :  230  accuracy =   0.5600  loss =  176.215\n",
      "testing  :  230  accuracy =   0.4463  loss =  179.469\n",
      "training :  231  accuracy =   0.5400  loss =  179.407\n",
      "testing  :  231  accuracy =   0.4471  loss =  179.419\n",
      "training :  232  accuracy =   0.5300  loss =  177.254\n",
      "testing  :  232  accuracy =   0.4508  loss =  179.061\n",
      "training :  233  accuracy =   0.4700  loss =  176.112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing  :  233  accuracy =   0.4592  loss =  178.579\n",
      "training :  234  accuracy =   0.5200  loss =  176.265\n",
      "testing  :  234  accuracy =   0.4660  loss =  178.285\n",
      "training :  235  accuracy =   0.5300  loss =  178.102\n",
      "testing  :  235  accuracy =   0.4701  loss =  178.053\n",
      "training :  236  accuracy =   0.4400  loss =  177.883\n",
      "testing  :  236  accuracy =   0.4705  loss =  177.965\n",
      "training :  237  accuracy =   0.4000  loss =  183.213\n",
      "testing  :  237  accuracy =   0.4742  loss =  177.957\n",
      "training :  238  accuracy =   0.4600  loss =  179.572\n",
      "testing  :  238  accuracy =   0.4991  loss =  177.885\n",
      "training :  239  accuracy =   0.5500  loss =  173.991\n",
      "testing  :  239  accuracy =   0.4605  loss =  177.844\n",
      "training :  240  accuracy =   0.4000  loss =  178.044\n",
      "testing  :  240  accuracy =   0.4547  loss =  177.874\n",
      "training :  241  accuracy =   0.4900  loss =  174.211\n",
      "testing  :  241  accuracy =   0.4552  loss =  177.998\n",
      "training :  242  accuracy =   0.4500  loss =  174.329\n",
      "testing  :  242  accuracy =   0.4542  loss =  178.107\n",
      "training :  243  accuracy =   0.5100  loss =  179.841\n",
      "testing  :  243  accuracy =   0.4541  loss =  178.156\n",
      "training :  244  accuracy =   0.4900  loss =  178.254\n",
      "testing  :  244  accuracy =   0.4547  loss =  178.176\n",
      "training :  245  accuracy =   0.3900  loss =  181.077\n",
      "testing  :  245  accuracy =   0.4547  loss =  178.047\n",
      "training :  246  accuracy =   0.5200  loss =  178.22\n",
      "testing  :  246  accuracy =   0.4569  loss =  177.87\n",
      "training :  247  accuracy =   0.3700  loss =  176.54\n",
      "testing  :  247  accuracy =   0.4548  loss =  177.676\n",
      "training :  248  accuracy =   0.4900  loss =  176.625\n",
      "testing  :  248  accuracy =   0.4516  loss =  177.618\n",
      "training :  249  accuracy =   0.4700  loss =  177.68\n",
      "testing  :  249  accuracy =   0.4552  loss =  177.626\n",
      "training :  250  accuracy =   0.5400  loss =  177.802\n",
      "testing  :  250  accuracy =   0.4586  loss =  177.622\n",
      "training :  251  accuracy =   0.4700  loss =  178.696\n",
      "testing  :  251  accuracy =   0.4620  loss =  177.712\n",
      "training :  252  accuracy =   0.5200  loss =  177.885\n",
      "testing  :  252  accuracy =   0.4666  loss =  177.778\n",
      "training :  253  accuracy =   0.4800  loss =  175.867\n",
      "testing  :  253  accuracy =   0.4715  loss =  177.754\n",
      "training :  254  accuracy =   0.4800  loss =  178.918\n",
      "testing  :  254  accuracy =   0.4753  loss =  177.755\n",
      "training :  255  accuracy =   0.5000  loss =  178.783\n",
      "testing  :  255  accuracy =   0.4764  loss =  177.601\n",
      "training :  256  accuracy =   0.4800  loss =  177.4\n",
      "testing  :  256  accuracy =   0.4747  loss =  177.277\n",
      "training :  257  accuracy =   0.4300  loss =  176.107\n",
      "testing  :  257  accuracy =   0.4853  loss =  177.079\n",
      "training :  258  accuracy =   0.5000  loss =  172.033\n",
      "testing  :  258  accuracy =   0.4948  loss =  176.955\n",
      "training :  259  accuracy =   0.5500  loss =  181.906\n",
      "testing  :  259  accuracy =   0.5047  loss =  176.911\n",
      "training :  260  accuracy =   0.5500  loss =  178.1\n",
      "testing  :  260  accuracy =   0.5096  loss =  176.928\n",
      "training :  261  accuracy =   0.4800  loss =  177.406\n",
      "testing  :  261  accuracy =   0.5130  loss =  176.869\n",
      "training :  262  accuracy =   0.5300  loss =  177.984\n",
      "testing  :  262  accuracy =   0.5173  loss =  176.722\n",
      "training :  263  accuracy =   0.5400  loss =  177.157\n",
      "testing  :  263  accuracy =   0.5194  loss =  176.624\n",
      "training :  264  accuracy =   0.5300  loss =  174.319\n",
      "testing  :  264  accuracy =   0.5233  loss =  176.523\n",
      "training :  265  accuracy =   0.6200  loss =  174.946\n",
      "testing  :  265  accuracy =   0.5244  loss =  176.472\n",
      "training :  266  accuracy =   0.5600  loss =  177.874\n",
      "testing  :  266  accuracy =   0.5260  loss =  176.479\n",
      "training :  267  accuracy =   0.5800  loss =  177.392\n",
      "testing  :  267  accuracy =   0.5271  loss =  176.506\n",
      "training :  268  accuracy =   0.5400  loss =  179.738\n",
      "testing  :  268  accuracy =   0.5270  loss =  176.561\n",
      "training :  269  accuracy =   0.5500  loss =  176.491\n",
      "testing  :  269  accuracy =   0.5252  loss =  176.595\n",
      "training :  270  accuracy =   0.4900  loss =  177.082\n",
      "testing  :  270  accuracy =   0.5242  loss =  176.605\n",
      "training :  271  accuracy =   0.4800  loss =  173.495\n",
      "testing  :  271  accuracy =   0.5223  loss =  176.561\n",
      "training :  272  accuracy =   0.5000  loss =  174.937\n",
      "testing  :  272  accuracy =   0.5201  loss =  176.556\n",
      "training :  273  accuracy =   0.6400  loss =  174.968\n",
      "testing  :  273  accuracy =   0.5188  loss =  176.476\n",
      "training :  274  accuracy =   0.4600  loss =  181.294\n",
      "testing  :  274  accuracy =   0.5202  loss =  176.363\n",
      "training :  275  accuracy =   0.5600  loss =  177.336\n",
      "testing  :  275  accuracy =   0.5229  loss =  176.143\n",
      "training :  276  accuracy =   0.5600  loss =  175.764\n",
      "testing  :  276  accuracy =   0.5253  loss =  175.994\n",
      "training :  277  accuracy =   0.5300  loss =  176.154\n",
      "testing  :  277  accuracy =   0.5269  loss =  175.893\n",
      "training :  278  accuracy =   0.5000  loss =  174.806\n",
      "testing  :  278  accuracy =   0.5259  loss =  175.892\n",
      "training :  279  accuracy =   0.6200  loss =  175.337\n",
      "testing  :  279  accuracy =   0.5250  loss =  175.901\n",
      "training :  280  accuracy =   0.5700  loss =  174.721\n",
      "testing  :  280  accuracy =   0.5202  loss =  175.904\n",
      "training :  281  accuracy =   0.6200  loss =  176.005\n",
      "testing  :  281  accuracy =   0.5163  loss =  175.967\n",
      "training :  282  accuracy =   0.5500  loss =  174.111\n",
      "testing  :  282  accuracy =   0.5060  loss =  175.959\n",
      "training :  283  accuracy =   0.4200  loss =  175.568\n",
      "testing  :  283  accuracy =   0.4972  loss =  175.901\n",
      "training :  284  accuracy =   0.4600  loss =  178.822\n",
      "testing  :  284  accuracy =   0.4728  loss =  175.816\n",
      "training :  285  accuracy =   0.4300  loss =  173.875\n",
      "testing  :  285  accuracy =   0.4595  loss =  175.782\n",
      "training :  286  accuracy =   0.4200  loss =  177.998\n",
      "testing  :  286  accuracy =   0.4519  loss =  175.731\n",
      "training :  287  accuracy =   0.4400  loss =  175.933\n",
      "testing  :  287  accuracy =   0.4487  loss =  175.695\n",
      "training :  288  accuracy =   0.4700  loss =  176.043\n",
      "testing  :  288  accuracy =   0.4483  loss =  175.661\n",
      "training :  289  accuracy =   0.4500  loss =  176.579\n",
      "testing  :  289  accuracy =   0.4470  loss =  175.62\n",
      "training :  290  accuracy =   0.4700  loss =  177.393\n",
      "testing  :  290  accuracy =   0.4451  loss =  175.603\n",
      "training :  291  accuracy =   0.4800  loss =  174.745\n",
      "testing  :  291  accuracy =   0.4444  loss =  175.559\n",
      "training :  292  accuracy =   0.4300  loss =  178.221\n",
      "testing  :  292  accuracy =   0.4438  loss =  175.51\n",
      "training :  293  accuracy =   0.3000  loss =  176.611\n",
      "testing  :  293  accuracy =   0.4457  loss =  175.406\n",
      "training :  294  accuracy =   0.4700  loss =  175.661\n",
      "testing  :  294  accuracy =   0.4478  loss =  175.348\n",
      "training :  295  accuracy =   0.4700  loss =  173.475\n",
      "testing  :  295  accuracy =   0.4498  loss =  175.386\n",
      "training :  296  accuracy =   0.4100  loss =  175.694\n",
      "testing  :  296  accuracy =   0.4525  loss =  175.324\n",
      "training :  297  accuracy =   0.5200  loss =  174.494\n",
      "testing  :  297  accuracy =   0.4613  loss =  175.374\n",
      "training :  298  accuracy =   0.4600  loss =  174.338\n",
      "testing  :  298  accuracy =   0.4686  loss =  175.415\n",
      "training :  299  accuracy =   0.5000  loss =  177.013\n",
      "testing  :  299  accuracy =   0.4688  loss =  175.369\n",
      "training :  300  accuracy =   0.4700  loss =  175.102\n",
      "testing  :  300  accuracy =   0.4708  loss =  175.224\n",
      "training :  301  accuracy =   0.4200  loss =  174.867\n",
      "testing  :  301  accuracy =   0.4712  loss =  175.146\n",
      "training :  302  accuracy =   0.5200  loss =  174.633\n",
      "testing  :  302  accuracy =   0.4704  loss =  175.05\n",
      "training :  303  accuracy =   0.5500  loss =  174.811\n",
      "testing  :  303  accuracy =   0.4748  loss =  174.963\n",
      "training :  304  accuracy =   0.5600  loss =  173.526\n",
      "testing  :  304  accuracy =   0.4797  loss =  174.954\n",
      "training :  305  accuracy =   0.4900  loss =  172.797\n",
      "testing  :  305  accuracy =   0.4846  loss =  174.921\n",
      "training :  306  accuracy =   0.4500  loss =  171.544\n",
      "testing  :  306  accuracy =   0.4877  loss =  174.905\n",
      "training :  307  accuracy =   0.4700  loss =  172.124\n",
      "testing  :  307  accuracy =   0.4923  loss =  174.992\n",
      "training :  308  accuracy =   0.5700  loss =  173.407\n",
      "testing  :  308  accuracy =   0.4971  loss =  175.075\n",
      "training :  309  accuracy =   0.5300  loss =  173.966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing  :  309  accuracy =   0.4981  loss =  175.142\n",
      "training :  310  accuracy =   0.5500  loss =  172.713\n",
      "testing  :  310  accuracy =   0.4974  loss =  175.133\n",
      "training :  311  accuracy =   0.5200  loss =  176.587\n",
      "testing  :  311  accuracy =   0.4986  loss =  175.202\n",
      "training :  312  accuracy =   0.5400  loss =  173.464\n",
      "testing  :  312  accuracy =   0.4964  loss =  175.181\n",
      "training :  313  accuracy =   0.5100  loss =  173.972\n",
      "testing  :  313  accuracy =   0.4930  loss =  174.937\n",
      "training :  314  accuracy =   0.5000  loss =  173.979\n",
      "testing  :  314  accuracy =   0.4871  loss =  174.732\n",
      "training :  315  accuracy =   0.4800  loss =  172.03\n",
      "testing  :  315  accuracy =   0.4691  loss =  174.551\n",
      "training :  316  accuracy =   0.4400  loss =  174.532\n",
      "testing  :  316  accuracy =   0.4685  loss =  174.498\n",
      "training :  317  accuracy =   0.3600  loss =  178.036\n",
      "testing  :  317  accuracy =   0.4709  loss =  174.417\n",
      "training :  318  accuracy =   0.5100  loss =  174.307\n",
      "testing  :  318  accuracy =   0.4735  loss =  174.314\n",
      "training :  319  accuracy =   0.4200  loss =  174.445\n",
      "testing  :  319  accuracy =   0.4677  loss =  174.217\n",
      "training :  320  accuracy =   0.4300  loss =  176.761\n",
      "testing  :  320  accuracy =   0.4625  loss =  174.153\n",
      "training :  321  accuracy =   0.4700  loss =  172.698\n",
      "testing  :  321  accuracy =   0.4564  loss =  174.071\n",
      "training :  322  accuracy =   0.5300  loss =  173.709\n",
      "testing  :  322  accuracy =   0.4541  loss =  174.02\n",
      "training :  323  accuracy =   0.4400  loss =  174.288\n",
      "testing  :  323  accuracy =   0.4544  loss =  173.987\n",
      "training :  324  accuracy =   0.4400  loss =  174.408\n",
      "testing  :  324  accuracy =   0.4525  loss =  173.946\n",
      "training :  325  accuracy =   0.4200  loss =  177.745\n",
      "testing  :  325  accuracy =   0.4506  loss =  173.915\n",
      "training :  326  accuracy =   0.4900  loss =  169.9\n",
      "testing  :  326  accuracy =   0.4490  loss =  173.901\n",
      "training :  327  accuracy =   0.4700  loss =  173.66\n",
      "testing  :  327  accuracy =   0.4473  loss =  173.92\n",
      "training :  328  accuracy =   0.5100  loss =  176.163\n",
      "testing  :  328  accuracy =   0.4449  loss =  173.957\n",
      "training :  329  accuracy =   0.3900  loss =  173.613\n",
      "testing  :  329  accuracy =   0.4459  loss =  173.94\n",
      "training :  330  accuracy =   0.4500  loss =  175.309\n",
      "testing  :  330  accuracy =   0.4460  loss =  173.924\n",
      "training :  331  accuracy =   0.4200  loss =  172.851\n",
      "testing  :  331  accuracy =   0.4491  loss =  173.895\n",
      "training :  332  accuracy =   0.3800  loss =  175.619\n",
      "testing  :  332  accuracy =   0.4523  loss =  173.911\n",
      "training :  333  accuracy =   0.3600  loss =  173.22\n",
      "testing  :  333  accuracy =   0.4565  loss =  173.91\n",
      "training :  334  accuracy =   0.4600  loss =  173.695\n",
      "testing  :  334  accuracy =   0.4637  loss =  173.942\n",
      "training :  335  accuracy =   0.5000  loss =  172.51\n",
      "testing  :  335  accuracy =   0.4727  loss =  174.013\n",
      "training :  336  accuracy =   0.6100  loss =  170.871\n",
      "testing  :  336  accuracy =   0.5036  loss =  174.086\n",
      "training :  337  accuracy =   0.4900  loss =  173.753\n",
      "testing  : "
     ]
    }
   ],
   "source": [
    "# encoding: UTF-8\n",
    "# original source : https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/tree/master/tensorflow-mnist-tutorial\n",
    "# 2018.12 : modified by Seungkwon Lee(kahnlee@naver.com)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflowvisu\n",
    "import mnistdata\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# neural network with 5 layers\n",
    "#\n",
    "# · · · · · · · · · ·          (input data, flattened pixels)       X [batch, 784]   # 784 = 28*28\n",
    "# \\x/x\\x/x\\x/x\\x/x\\x/       -- fully connected layer (sigmoid)      W1 [784, 200]      B1[200]\n",
    "#  · · · · · · · · ·                                                Y1 [batch, 200]\n",
    "#   \\x/x\\x/x\\x/x\\x/         -- fully connected layer (sigmoid)      W2 [200, 100]      B2[100]\n",
    "#    · · · · · · ·                                                  Y2 [batch, 100]\n",
    "#     \\x/x\\x/x\\x/           -- fully connected layer (sigmoid)      W3 [100, 60]       B3[60]\n",
    "#      · · · · ·                                                    Y3 [batch, 60]\n",
    "#       \\x/x\\x/             -- fully connected layer (sigmoid)      W4 [60, 30]        B4[30]\n",
    "#        · · ·                                                      Y4 [batch, 30]\n",
    "#         \\x/               -- fully connected layer (softmax)      W5 [30, 10]        B5[10]\n",
    "#          ·                                                        Y5 [batch, 10]\n",
    "\n",
    "# Download images and labels into mnist.test (10K images+labels) and mnist.train (60K images+labels)\n",
    "mnist = mnistdata.read_data_sets(\"data\", one_hot=True, reshape=False)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 3)\n",
    "\n",
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "# correct answers will go here\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#neuron 개수\n",
    "# five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
    "L = 200\n",
    "M = 100\n",
    "N = 60\n",
    "O = 30\n",
    "# Weights initialised with small random values between -0.2 and +0.2\n",
    "# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
    "W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\n",
    "B1 = tf.Variable(tf.zeros([L]))\n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.zeros([M]))\n",
    "W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([N]))\n",
    "W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "B4 = tf.Variable(tf.zeros([O]))\n",
    "W5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# The model\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\n",
    "Y3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + B3)\n",
    "Y4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + B4)\n",
    "Ylogits = tf.nn.sigmoid(tf.matmul(Y4, W5) + B5)\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# training step, learning rate = 0.003\n",
    "learning_rate = 0.003\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "# run\n",
    "for i in range(10000 + 1) :\n",
    "\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    a, c = sess.run([accuracy, cross_entropy], feed_dict={X : batch_X, Y_ : batch_Y})\n",
    "    print(\"training : \", i, ' accuracy = ', '{:7.4f}'.format(a), ' loss = ', c)\n",
    "    train_acc_list.append(a)\n",
    "    train_loss_list.append(c)\n",
    "\n",
    "    # test_batch_X, test_batch_Y = mnist.test.next_batch(100)  ==> never use mini batch!!\n",
    "    # sess.run(train_step, feed_dict={X: test_batch_X, Y_: test_batch_Y})  ==> never run train_step on test data!!\n",
    "    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "    print(\"testing  : \",i, ' accuracy = ', '{:7.4f}'.format(a), ' loss = ', c)\n",
    "    test_acc_list.append(a)\n",
    "    test_loss_list.append(c)\n",
    "\n",
    "    sess.run(train_step, feed_dict={X : batch_X, Y_ : batch_Y} )\n",
    "    \n",
    "    if(i%500 ==0 and i!=0):\n",
    "        saver.save(sess, '../model/mnist_HL5_sigmoid')\n",
    "\n",
    "# draw graph : accuracy\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.figure(1) \n",
    "plt.plot(x, train_acc_list,  label='train', markevery=1)\n",
    "plt.plot(x, test_acc_list, label='test', markevery=1)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "# plt.show()\n",
    "\n",
    "# draw graph : loss\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.figure(2) \n",
    "plt.plot(x, train_loss_list,  label='train', markevery=1)\n",
    "plt.plot(x, test_loss_list, label='test', markevery=1)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 100)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
